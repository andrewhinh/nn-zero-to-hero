{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "- https://openai.com/index/chatgpt/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "class ParallelMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.n_head = num_heads\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.head_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q, k, v  = self.c_attn(x).split(n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, nh, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, nh, T, T)\n",
    "        wei = self.head_dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa = ParallelMultiHeadAttention(n_head)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-\\*/. Not an easy problem. You may need Chain of Thought traces.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79800 ['97*73=1807', '87*54=8964', '54-48=6', '37/91=14.0', '16+83=99']\n",
      "['*', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '='] 16\n",
      "torch.Size([787163])\n"
     ]
    }
   ],
   "source": [
    "# generate all possible 1-digit addition problems as strings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "sep = '.'\n",
    "combs = list(itertools.product(range(100), range(100)))\n",
    "procombsblems = np.array(combs)\n",
    "random.shuffle(combs)\n",
    "problems = []\n",
    "for a, b in combs:\n",
    "    problems.append(f\"{a}+{b}={str(a+b)[::-1]}\")\n",
    "    problems.append(f\"{b}+{a}={str(a+b)[::-1]}\")\n",
    "    problems.append(f\"{a}-{b}={str(a-b)[::-1]}\")\n",
    "    problems.append(f\"{b}-{a}={str(b-a)[::-1]}\")\n",
    "    problems.append(f\"{a}*{b}={str(a*b)[::-1]}\")\n",
    "    problems.append(f\"{b}*{a}={str(a*b)[::-1]}\")\n",
    "    if b != 0:\n",
    "        problems.append(f\"{a}/{b}={str(a/b)[::-1]}\")\n",
    "    if a != 0:\n",
    "        problems.append(f\"{b}/{a}={str(b/a)[::-1]}\")\n",
    "\n",
    "text = sep.join(problems)\n",
    "print(len(problems), random.sample(problems, 5))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars, vocab_size)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "\n",
    "# data loading\n",
    "def get_batch():\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # mask input tokens in the target sequence\n",
    "    for i, seq in enumerate(y):\n",
    "        last_sep_idx = 0\n",
    "        for j, token in enumerate(seq):\n",
    "            if token == stoi['=']:\n",
    "                y[i, last_sep_idx:j+1] = -100  # F.cross_entropy ignore_index default\n",
    "            elif token == stoi['.']:\n",
    "                last_sep_idx = j\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        _, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 11, 12,  ...,  5,  3, 14],\n",
       "        [ 4, 13,  6,  ..., 13,  6,  1],\n",
       "        [ 2,  3, 13,  ..., 15, 10,  3],\n",
       "        ...,\n",
       "        [ 7, 14, 15,  ..., 15, 11,  6],\n",
       "        [ 7,  1,  8,  ..., 10,  2, 12],\n",
       "        [ 8, 14,  4,  ..., 10,  5,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100,  ...,    3,   14,    6],\n",
       "        [-100, -100, -100,  ...,    6,    1,    8],\n",
       "        [-100, -100, -100,  ...,   10,    3,    6],\n",
       "        ...,\n",
       "        [-100, -100,    9,  ...,   11,    6,    2],\n",
       "        [-100, -100, -100,  ...,    2,   12,    7],\n",
       "        [-100, -100, -100,  ...,    5,    2,    6]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.591568 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        loss = estimate_loss(model)\n",
    "        print(f\"step {iter}: loss {loss:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch()\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context[0, 0] = stoi['1']\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c184ea43c64a088b905a950425f0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a84ba3f444048c39bdc029b28062746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/000_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599a674debdb4d0597dd96cc50903b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f838adcb70e440239edb5382ebeb27bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa5d644620d4e41941007ab7d2cb0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/003_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/003_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40cd5a58f2848b08f252fb2c0bf1924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a88498714d4e69bd9e2e1d553c2874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/005_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106501775bc64a9d83fcf18727a6a938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a3c34cc2654c179121d53012ddd305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/007_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/007_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8077a0519b07477b8f7a0bc8a60a799c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5cc7f5da3a4546a1063f0dfcadc62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/009_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7ad636124a427ba3f7ca694b2e9e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e897dceafa64b31a9dea3c18ba7e085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/011_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892c9878200f43a38ebc912ab69c9b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f766104adf4564824684f13f5e7a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/sample/10BT/013_00000.parquet: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62657c5dcd9c44c9b0375be466f51157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9672101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8442469840cd48eaa9945a307328d883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence of thought and the freedom to choose.\\nElizabeth’s refusal of Mr. Collins offer of marriage showed an independence seldom seen in heroines of the day. Her refusal of Mr. Darcy while triggered by anger showed a level of independence that left him shocked and stunned.\\nThe freedom she exhibited in finally accepting him in direct defiance of Lady Catherine and knowing her father would disapprove was unusual even for Austen. In her last book Anne Elliot is persuaded to refuse Captain Wentworth at Lady Russel’s insistence.\\nAlthough Jane played by the rules of the day, all of her writing is infused with how she wanted life to be. She ‘screams’ her outrage at the limitations for women in Emma.\\nWhen accosted by Mrs. Elton, Jane Fairfax says,\\n“Excuse me, ma’am, but this is by no means my intention; I make no inquiry myself, and should be sorry to have any made by my friends. When I am quite determined as to the time, I am not at all afraid of being long unemployed. There are places in town, offices, where inquiry would soon produce something — offices for the sale, not quite of human flesh, but of human intellect.”\\n“Oh! my dear, human flesh! You quite shock me; if you mean a fling at the slave-trade, I assure you Mr. Suckling was always rather a friend to the abolition.”\\n“I did not mean, I was not thinking of the slave-trade,” replied Jane; “governess-trade, I assure you, was all that I had in view; widely different certainly, as to the guilt of those who carry it on; but as to the greater misery of the victims, I do not know where it lies.”\\nThat same sentiment is emphasized in Emma’s shock when Mrs. Weston tells her of Frank Churchill’s secret engagement to Jane.\\n“Good God!” cried Emma, “Jane actually on the point of going as governess! What could he mean by such horrible indelicacy? To suffer her to engage herself — to suffer her even to think of such a measure!”\\nI find it interesting that at the moment of Austen’s birth or there about, John Adams left his farm in Massachusetts for the Continental Congress in Philadelphia. Doesn’t sound particularly interesting, I know but consider this.\\nJohn Adams left his home in mid-December 1775 to attend an unprecedented meeting of colonial representatives to consider severing ties with their mother country and her monarch; a decision that culminated in a document unlike any ever written. In the mother country, one day in that same cold December a baby girl was born at Steventon Rectory. Her cry was heard by only the people in the house but the years to come would see her pen create works unlike any the world had ever seen.\\nComparing Austen’s words with Thomas Jefferson’s may seem a trivialization but I believe that Austen’s impact on the world is no less important than Jefferson’s. The effect of Jane’s writing maybe more subtle than that of the Virginian but it is no less influential.\\nJefferson’s words instigated and promoted a revolution, a war of independence. Jane’s words had no such excessive consequence. Still in her own quiet, genteel yet powerful way she declared and promoted the same principles of freedom and self-regulated independence as our American forefathers. In all her novels Jane advocates independence of person and thought, the rights of all and acceptance of responsibility for those rights.\\nJane may not have incited military action as Jefferson did but even as an avowed royalist, I doubt not that Jane Austen firmly believed in his declaration of the right to life, liberty and the pursuit of happiness.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0][\"text\"], ds['train'][:2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 [\"In this section we will learn Spring IoC with the help of many articles and ready to test example code. In this section we are exploring IOC container of the Spring 3 framework. The IOC container is the main component of the Spring framework. It provides the main IoC container and AOP framework. The core container of the Spring Framework provides important functionality including dependency injection and bean lifecycle management.\\nThe core container is responsible for providing essential functionality to the Spring framework. The BeanFactory is the primary component of the core container. The BeanFactory is an implementation of the Factory pattern. The core container of the Spring Framework provides Dependency Injection and Inversion of Control (IOC) functionalities.\\nModules of Core Container:\\nFollowing are the modules of the Spring Core Container:\\nThe IoC or Inversion of Control is the core features of the Spring Framework. Developers uses the IoC container to manage the beans and its dependency in the application. Thus simplifies the implementation of business logic in the application. The IoC is very important and it's very necessary to fully understand.\\nWhat is IoC?\\nThe Inversion of Control is the process by which application defines the dependency and these dependencies are then satisfied in runtime by the Spring Framework. The IoC is also known as Dependency Injection(DI).\\nI the application dependencies are satisfied through:\\nIn this section we will be presenting the examples of Spring Core module.Spring Bean Example, Spring Bean Creation\\nIf you are facing any programming issue, such as compilation errors or not able to find the code you are looking for.\\nAsk your questions, our development team will try to give answers to your questions.\", 'McLaren Lapeer Region Diagnostic Tests Get to the Heart of the Matter\\nThe hospital’s new Cardiac Catheterization Lab is a valuable tool in diagnosing heart conditions, especially coronary heart disease. As cardiac catheterization is not the first test completed, a patient may have one or more non-invasive tests that are also tremendously important in determining the cause of heart dysfunction.\\nThe most common tests utilized at McLaren Lapeer Region to diagnose heart conditions are electrocardiogram (EKG), chest X-ray, echocardiogram, and stress tests.\\nThe standard chest X-ray provides an image of the heart structure and helps in assessing the heart’s condition.\\nThe electrocardiogram produces a record or electrical activity of the heart. It is used to evaluate for heart arrhythmias, diagnose heart attack, and check an assortment of abnormalities. To substantiate a diagnosis of a heart problem, the patient may be asked to wear an EKG holter monitor for 24 or 48 hours. The patient’s heart rhythm is recorded as they go about their daily activities. This allows a correlation to be made against possible arrhythmias. Computer downloads the test and the cardiologist reviews the final report.\\nAnother test that enhances cardiac diagnosis is the echocardiogram. An echocardiogram is an ultrasound examination of the heart that allows the sonographer to view and record the blood flow that is pumped through the chambers of the heart. The echocardiogram is utilized as a stand-alone test or may be done in conjunction with stress testing.\\nA stress test can be conducted to reveal coronary heart disease; blood flow and muscle function, and identify abnormal heart rhythms. The stress test also helps physicians determine how effective a medication might be in controlling a heart condition. A stress test typically involves a patient exercising on a treadmill though even a person who is unable to do this can be evaluated using medication to stimulate the heart and simulate exercise. A thallium stress test done in nuclear medicine uses radioactive material to enable images of the heart to be taken. The images help to evaluate the health and function of the heart.\\nCardiac enzyme analysis of blood through the Clinical Laboratory is very helpful in determining the severity of a heart attack and when it occurred.\\nIn addition to these exams, McLaren Lapeer Region patients have access to electro physiology testing at McLaren Flint Heart and Vascular Center to determine what might be causing arrhythmias and to EBT testing that can determine calcium levels in the coronary arteries.', \"The green team was formed in 2015 with the mandate to make the school environmentally aware and to begin initiatives that reduced the amount of wastage in our school. The programs we started and plan on running in the future are all meant to teach our students how they can help the environment. It is hoped that what they learn in school about the environment will be taken home and shared with family. Often, it's the little things in life that make the biggest difference.\\nIn our first year we started a recycling program that introduced the staff and students to the idea of recycling many of the items they would normally throw away. We educated the students on why we recycle and how important the process is to the continued health of the environment. We held several contests that promoted recycling, reusing, and reducing. The winners won class prizes and had their projects displayed for all to see in front of the office.\\nAnother Green Team project we started was a vegetable garden. Students started growing the plants from seeds in the classroom and a month later they were transplanted into larger growing containers. The seeds were then planted in the Elders Court of the school where they continued to grow until late September when they were harvested.\\nWith the success of the small garden we have hopes of getting a grant to create a much larger garden located on or near school grounds. We have plans of partnering with the Norway House Band and community members to ensure the success of this project. With the support of the community we hope to have dedicated volunteers who will work the garden during the summer months when the school is closed and staff are off on holidays. In return for their help they will be able to take most of the vegetables to share with their families. The school will take enough for our harvest feast that we hope will become an annual occurrence.\\nWe also are looking to do some beekeeping near the school. Other northern communities have already started beekeeping and are showing a lot of success. Some are getting enough honey to sell and the money goes back into the program. We hope to do the same in our school. We plan on using the beekeeping as an educational resource for the students. We feel that it is critical for our students to learn about bees and how important they are to the welfare of our environment.\", 'Mayan Calendar Discovery\\nBy John Van Auken\\nArchaeologists have found the underground chamber of an official Mayan calendar chronicler! And what his inscriptions reveal is that there is another Age following the present \"Age of Movement\" that ends on this coming Winter Solstice, December 21, 2012. The world does not end—Hooray! In fact, it goes on and on and on. William Saturno of Boston University, the archaeologist who manages the new discovery, published an article in the journal Science, and writes: \"The Maya calendar is going to keep going and keep going for billions, trillions, octillions of years into the future, a huge number that we can\\'t even wrap our heads around.\"\\nTyrone Turner, National Geographic\\nArchaeologist William Saturno of Boston University excavates a house in the ruins of the Maya city of Xultun.\\nWhat Saturno found turned out to be a well-preserved mural that includes the oldest known Mayan calendar ever found. And just like the Maya Long Count calendar, which serves as the basis for the apocalypse myth, this calendar extends indefinitely into the future.\\nThe Mayan calendar is 20 katun cycles containing 144,000 days, equal to 394.26 tropical years, and a whole cycle is 5,128 years. The current cycle we\\'ve been living in began on August 11 or 13, 3114 BC, and ends this year on December 21 or 23. The current 13th baktun will end, or be completed, on the Mayan date of 22.214.171.124.0 (which is equivalent to December 21, 2012 using the GMT correlation).\\nThis new discovery provides us with clear evidence that the end of this 5,128 cycle marks the beginning of a new cycle! According to Mayan legend, the current world—the one in which we are all currently living—is an era of major change, the so-called Age of Movement.\\nCuriously, the newly discovered Mayan calendar in the chronicler\\'s chamber has cycles of time recording 17 baktuns, rather than the standard 13.\\nThe new finding shows that the calculations include a time span longer than 6,000 years that could extend well beyond 2012. Anthony Aveni of Colgate University in Hamilton, N.Y., an expert on Mayan astronomy asks, \"Why would they go into those numbers if the world is going to come to an end this year? You could say a number that big at least suggests that time marches on.\"\\nLeaders among the Maya were keenly interested in astronomy and sought to coordinate sacred rituals with events in the heavens above. A chronicler of the heaven movements and time cycles would have been an honored member of the Maya court.\\nThe wall in the chronicler\\'s chamber was used in the same way a modern mathematician might use a whiteboard. The chronicler wrote his frequently consulted formulas on the wall of his chamber instead of having to look them up in a book. And— because these calendar details were inscribed on a wall—he preserved them better than any book would have. In fact, no books remain from the period when the chronicler wrote on his wall—researchers believe this wall dates to 800 AD.\\nIn addition to the time data, there are pictures on the other walls in the chronicler\\'s chamber, including an image of a king in a feather headdress, seated on a throne, with a white-garbed person peeking out from behind him (possibly a self-portrait of our chronicler). A painting of a scribe holding a stylus was on another wall.\\nAccording to the archaeologists, these paintings were the first Maya art to be found on the walls of a person\\'s house.!\\nThe chamber is a little larger than 6-feet square and is part of a larger complex in Mayan ruins of the rain forest at Xultun in northeastern Guatemala. Xultun is a large Early Classic Maya site. It is 24 miles northeast of the more famous Mayan site of Tikal. The site contains a pyramid that is 114 feet high, 2 ball courts, 24 stelae, and 5 reservoirs (aguadas).\\nimage courtesy http://people.tribe.net\\nAnthony Aveni said the astronomical data on the wall would allow a chronicler to predict the appearance of a full moon years in advance and could be used to advise the king on when to go to war or how good this year\\'s crops would be. \"What you have here is astronomy driven by religion,\" Aveni said.\\nOn an adjacent wall are numbers indicating four time spans from roughly 935 to 6,700 years. It\\'s not clear what they represent, but maybe the chronicler was doing calculations that combined observations from important astronomical events like the movements of Mars, Venus, and the moon, the researchers said. Why bother to do that? Aveni suggest that perhaps the chronicler and his colleagues were \"geeks ... who just got carried away with doing these kinds of computations and calculations, and probably did them far beyond the needs of ordinary society.\"\\nSimon Martin, co-curator of an exhibit about the Mayan calendar at the University of Pennsylvania Museum of Archaeology and Anthropology is very excited by this find: \"It\\'s really a wonderful surprise. We\\'ve never really been able to identify a working space, or how they actually went about things.\" The new work gives insight into how the Maya worked on their calculations, he said, and the fact that the room had a stone roof rather than thatching supports previous indications that the chronicler enjoyed a high social standing.\\nAn undergraduate student working with Saturno\\'s team in 2010 first glimpsed the chronicler\\'s chamber. After first dismissing the value of a little piece of paint on a stone wall spotted by his student, Saturno later went back to the spot and dug deeper, finding the amazing timekeeper\\'s chamber. To Saturno\\'s amazement, the wall writings were of immeasurable value to the growing knowledge of the Maya and their fascination with the stars, planets, and the passage of time.\\nJohn Carlson, director of the Center for Archaeoastronomy in College Park, Md., said, \"It\\'s a very important discovery. We\\'re only getting a glimpse of the story\" in the published paper. Complete details and photos of the discovery will be published in the June issue of National Geographic, which funded some of this research.\\nJohn Van Auken, a director at the Association for Research and Enlightenment, is an acknowledged expert on the Cayce readings, the Bible, ancient prophecies, world religions, meditation, and ancient Egypt. John conducts seminars in the U.S. and abroad, and is a tour guide to the many sacred sites around the world with A.R.E. Travel. He is the author of numerous books, including Edgar Cayce and the Kabbalah, Ancient Egyptian Mysticism and From Karma to Grace. He will be presenting at several upcoming Virginia Beach Conferences including the Annual Ancient Mysteries Conference: Ancient Mysteries Conference—Digging for the Truth with Dr. Zahi Hawass from October 4-7, 2012.', 'Tuesday, October 20\\nDrawing and (Trying) to Achieve a Correct Grasp\\nI was recently watching one of the Montessori videos by Margaret Humfray. In her lecture she talked about the fact that children should be taught the correct way to use any tool or material and that they should only use it if they can use it the correct way. Among these tools she includes pencils and anything else used for writing or drawing. The reason for only giving the tool to a child who can use it the correct way, she states, is because the child will develop a habit of using/holding the tool the incorrect way, which can cause difficulties later on and will even require the child to relearn how to use it. After a little internet searching, I found Margaret\\'s statements to be relevant. School occupational therapists report incorrect pencil grasps as one of the most common problems they are consulted about. Listed as one reason for incorrect grasp is writing before the hand is developmentally ready for the activity. (Check out this website for a list of activities to promote readiness-many of which are used in the Montessori classroom.)\\nRight away I started watching my oldest daughter using her favorite writing tool: markers. Although she has beeswax block crayons and Lyra colored pencils, she usually will choose the markers if they are available. I have noticed in the past that sometimes she will use a tripod grasp when drawing and other times she uses a whole hand grasp, but I had not considered it related to the writing utensil she was using, until now.\\nThe picture above and this picture demonstrate the grasp she uses with a fat Crayola marker. Neither of these grasps are correct and uses more fingers than the correct grasp.\\nThis picture shows her grasp using a Melissa and Doug triangular crayon. This grasp is much closer to the correct \"dynamic tripod\" grasp. Although I have introduced holding a pencil with this grasp I didn\\'t mention any of it for these pictures, I just invited her to draw with all of them so I could see the natural grasps. I was really surprised to see the grasps changed so frequently.\\nAfter thinking about all of this information I wondered: What is a parent to do? My children love drawing, I want them to develop this form of creativity but I don\\'t want to be providing something they are not developmentally ready for and something that could in fact be creating a future obstacle. In Montessori education children draw letters and numbers they are learning in the sand. I wondered about using the sand tray for drawing. We tried it but for drawing purposes it didn\\'t provide the creativity and I think we all felt it was limiting. For now will stick with the sand tray for letters and numbers.\\nFor other options I checked online to see what is available. I have seen the crayon rocks before but never tried them. Something small, like the crayon rocks or small pieces of chalk would encourage a tripod grasp.\\nI am also interested in getting a pencil grip. They look like something that would really help.\\nIn thinking about all this new information, I keep returning to a reader comment made in the post about letter reversal basically saying we sometimes give children \" too much, too soon.\" I thought that was a very accurate statement. Writing this post has made me really look at what I am providing and if it is something I want to provide, how I can provide the most developmentally appropriate version of it.']\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '~', '\\xa0', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'ĉ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'ĥ', 'Ħ', 'ħ', 'ĩ', 'Ī', 'ī', 'ĭ', 'į', 'İ', 'ı', 'ĵ', 'Ķ', 'ķ', 'Ĺ', 'ĺ', 'ļ', 'Ľ', 'ľ', 'ŀ', 'Ł', 'ł', 'Ń', 'ń', 'ņ', 'ň', 'ŉ', 'Ŋ', 'ŋ', 'Ō', 'ō', 'ŏ', 'Ő', 'ő', 'Œ', 'œ', 'Ŕ', 'ŕ', 'ŗ', 'Ř', 'ř', 'Ś', 'ś', 'ŝ', 'Ş', 'ş', 'Š', 'š', 'Ţ', 'ţ', 'Ť', 'ť', 'ũ', 'Ū', 'ū', 'Ŭ', 'ŭ', 'Ů', 'ů', 'ű', 'ų', 'ŵ', 'ŷ', 'Ÿ', 'Ź', 'ź', 'Ż', 'ż', 'Ž', 'ž', 'ſ', 'ƈ', 'Ɖ', 'Ǝ', 'ƒ', 'ƕ', 'ƛ', 'ơ', 'Ƥ', 'ƥ', 'Ư', 'ư', 'ƴ', 'ǀ', 'ǁ', 'ǂ', 'ǃ', 'ǎ', 'ǐ', 'ǒ', 'ǔ', 'ǖ', 'ǜ', 'ǝ', 'ǣ', 'Ǧ', 'ǧ', 'ǫ', 'ǰ', 'ǵ', 'ǽ', 'ǿ', 'ȇ', 'ȋ', 'Ș', 'ș', 'ț', 'Ȝ', 'ȝ', 'ȟ', 'Ȣ', 'ȧ', 'ȯ', 'ȳ', 'ȶ', 'ɐ', 'ɑ', 'ɒ', 'ɔ', 'ɕ', 'ɖ', 'ə', 'ɚ', 'ɛ', 'ɜ', 'ɝ', 'ɟ', 'ɡ', 'ɢ', 'ɣ', 'ɤ', 'ɦ', 'ɨ', 'ɩ', 'ɪ', 'ɫ', 'ɬ', 'ɭ', 'ɮ', 'ɯ', 'ɰ', 'ɱ', 'ɲ', 'ɳ', 'ɴ', 'ɶ', 'ɸ', 'ɹ', 'ɻ', 'ɽ', 'ɾ', 'ʀ', 'ʁ', 'ʂ', 'ʃ', 'ʆ', 'ʉ', 'ʊ', 'ʋ', 'ʌ', 'ʏ', 'ʐ', 'ʒ', 'ʔ', 'ʕ', 'ʙ', 'ʜ', 'ʝ', 'ʟ', 'ʢ', 'ʤ', 'ʧ', 'ʰ', 'ʲ', 'ʷ', 'ʹ', 'ʺ', 'ʻ', 'ʼ', 'ʽ', 'ʾ', 'ʿ', 'ˀ', '˂', 'ˆ', 'ˇ', 'ˈ', 'ˉ', 'ˊ', 'ˌ', 'ː', 'ˑ', '˒', '˓', '˗', '˘', '˙', '˚', '˛', '˜', '˝', 'ˠ', 'ˡ', 'ˤ', '̀', '́', '̂', '̃', '̄', '̅', '̆', '̇', '̈', '̊', '̌', '̑', '̓', '̕', '̙', '̚', '̜', '̝', '̞', '̟', '̠', '̣', '̥', '̧', '̩', '̪', '̯', '̰', '̱', '̳', '̴', '̶', '̹', '͕', '͞', '͡', 'ͦ', '͵', 'ͻ', '΄', 'Ά', 'Έ', 'Ή', 'Ό', 'ΐ', 'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ξ', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω', 'Ϊ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ϊ', 'ϋ', 'ό', 'ύ', 'ώ', 'ϑ', 'ϕ', 'Ϛ', 'Ϝ', 'ϝ', 'ϟ', 'ϡ', 'ϰ', 'ϱ', 'Ђ', 'І', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ѐ', 'ё', 'є', 'ѕ', 'і', 'ї', 'ј', 'њ', 'ћ', 'ќ', 'ў', 'ѣ', 'ѧ', 'ѳ', 'ѵ', 'Ѻ', 'ґ', 'Ғ', 'ғ', 'Қ', 'қ', 'Ң', 'ң', 'Ү', 'ү', 'ұ', 'һ', 'Ӓ', 'ӓ', 'Ӕ', 'ӕ', 'ә', 'ӧ', 'Ө', 'ө', 'Բ', 'Գ', 'Հ', 'Ն', 'Պ', 'Ջ', 'ա', 'բ', 'գ', 'դ', 'ե', 'է', 'թ', 'ի', 'լ', 'ծ', 'կ', 'հ', 'ղ', 'մ', 'յ', 'ն', 'շ', 'ո', 'պ', 'ջ', 'ս', 'տ', 'ր', 'ց', 'ւ', 'ք', '֑', '֖', '֗', '֞', '֥', 'ְ', 'ֱ', 'ֲ', 'ֳ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֻ', 'ּ', 'ֽ', '־', 'ׁ', 'ׂ', '׃', 'ׇ', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך', 'כ', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת', '،', '؏', '؛', '؟', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ـ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٔ', '٠', '١', '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩', 'ٰ', 'ٱ', 'ٹ', 'ټ', 'پ', 'چ', 'ډ', 'ڌ', 'ړ', 'ژ', 'ڨ', 'ک', 'گ', 'ں', 'ھ', 'ۀ', 'ہ', 'ۇ', 'ی', 'ۍ', 'ێ', 'ې', 'ے', 'ۓ', '۔', 'ە', 'ۖ', 'ۗ', '۞', '۟', 'ۦ', '۴', '۸', 'ܐ', 'ܘ', 'ܝ', 'ܢ', 'ݣ', 'ބ', 'އ', 'ވ', 'ލ', 'ގ', 'ޑ', 'ަ', 'ި', 'ު', 'ޮ', 'ߘ', 'ࢹ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॐ', '॑', '॒', '।', '॥', '०', '१', '२', '४', '५', '९', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঋ', 'এ', 'ও', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঞ', 'ট', 'ঠ', 'ড', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', '্', 'ৎ', '০', '১', '২', '৪', '৭', 'ৰ', 'ਂ', 'ਃ', 'ਅ', 'ਆ', 'ਇ', 'ਈ', 'ਉ', 'ਊ', 'ਏ', 'ਐ', 'ਓ', 'ਔ', 'ਕ', 'ਖ', 'ਗ', 'ਙ', 'ਚ', 'ਛ', 'ਜ', 'ਞ', 'ਟ', 'ਤ', 'ਦ', 'ਧ', 'ਨ', 'ਪ', 'ਫ', 'ਬ', 'ਭ', 'ਮ', 'ਰ', 'ਲ', 'ਵ', 'ਸ', 'ਹ', '਼', 'ਾ', 'ਿ', 'ੀ', 'ੁ', 'ੂ', 'ੇ', 'ੈ', 'ੋ', 'ੌ', '੍', 'ੑ', 'ੜ', 'ੰ', 'ੱ', 'ੲ', 'ੳ', 'ੴ', 'ં', 'અ', 'ક', 'ગ', 'જ', 'ઞ', 'ઠ', 'ડ', 'ઢ', 'ત', 'થ', 'દ', 'ધ', 'ન', 'પ', 'ફ', 'ભ', 'મ', 'ય', 'ર', 'લ', 'વ', 'શ', 'ષ', 'સ', 'હ', '઼', 'ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ે', 'ૈ', 'ો', '્', 'ଗ', 'ଚ', 'ଜ', 'ଣ', 'ତ', 'ଦ', 'ପ', 'ବ', 'ମ', 'ର', 'ା', 'ି', 'ୁ', 'ୂ', '୍', 'ୟ', 'அ', 'ஆ', 'இ', 'உ', 'எ', 'ஒ', 'க', 'ங', 'ச', 'ஜ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ோ', '்', 'ం', 'అ', 'ఆ', 'క', 'గ', 'డ', 'ణ', 'త', 'ద', 'న', 'ప', 'బ', 'భ', 'మ', 'య', 'ర', 'ల', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ె', 'ే', 'ై', 'ో', '్', 'ಂ', 'ಅ', 'ಕ', 'ಗ', 'ಚ', 'ಜ', 'ಟ', 'ಡ', 'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', 'ಪ', 'ಬ', 'ಭ', 'ಮ', 'ಯ', 'ರ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೆ', 'ೇ', 'ೈ', '್', 'ക', 'ഗ', 'ണ', 'ത', 'ന', 'പ', 'ഭ', 'മ', 'യ', 'ര', 'ള', 'ഷ', 'സ', 'ാ', 'ി', 'ു', 'ൂ', 'േ', '്', 'ං', 'ඉ', 'ග', 'ජ', 'ත', 'ද', 'ප', 'ල', 'හ', 'ළ', 'ා', 'ි', 'ก', 'ข', 'ค', 'ง', 'จ', 'ช', 'ซ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ณ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'อ', 'ฯ', 'ะ', 'ั', 'า', 'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'เ', 'แ', 'โ', 'ใ', 'ไ', '็', '่', '้', '์', 'ກ', 'ຄ', 'ງ', 'ຊ', 'ດ', 'ຕ', 'ຖ', 'ທ', 'ນ', 'ບ', 'ປ', 'ພ', 'ມ', 'ລ', 'ວ', 'ສ', 'ຫ', 'ອ', 'ະ', 'ັ', 'າ', 'ຳ', 'ິ', 'ຼ', 'ຽ', 'ເ', 'ໄ', '່', '་', '།', 'ཀ', 'ཁ', 'ག', 'ང', 'ཆ', 'ཇ', 'ཉ', 'ཏ', 'ཐ', 'ད', 'ན', 'པ', 'བ', 'མ', 'ཙ', 'ཚ', 'ཛ', 'ཞ', 'འ', 'ཡ', 'ར', 'ལ', 'ཤ', 'ས', 'ི', 'ུ', 'ེ', 'ོ', 'ྒ', 'ྟ', 'ྤ', 'ྱ', 'ྷ', 'ྼ', 'က', 'ခ', 'ဂ', 'င', 'စ', 'ဆ', 'တ', 'န', 'ပ', 'ဖ', 'မ', 'ယ', 'ရ', 'သ', 'ဦ', 'ာ', 'ိ', 'ု', 'ူ', 'ေ', 'ဲ', 'ံ', '့', 'း', '်', 'ြ', 'ွ', 'ှ', 'ა', 'ბ', 'გ', 'დ', 'ე', 'ვ', 'ზ', 'თ', 'ი', 'კ', 'ლ', 'მ', 'ნ', 'ო', 'პ', 'ჟ', 'რ', 'ს', 'ტ', 'უ', 'ფ', 'ქ', 'ღ', 'ყ', 'შ', 'ჩ', 'ც', 'ძ', 'წ', 'ჭ', 'ხ', 'ჯ', 'ჰ', 'ჵ', 'ჺ', 'ᅟ', 'ህ', 'ለ', 'ሊ', 'ላ', 'ሚ', 'ማ', 'ረ', 'ር', 'ስ', 'ቅ', 'ቤ', 'ብ', 'ተ', 'ት', 'ቶ', 'ች', 'ኅ', 'ና', 'ን', 'አ', 'እ', 'ካ', 'ክ', 'ወ', 'ው', 'ዝ', 'የ', 'ይ', 'ደ', 'ዱ', 'ዲ', 'ድ', 'ጊ', 'ግ', 'ጨ', 'ፃ', 'ፈ', 'Ꭲ', 'Ꭴ', 'Ꭺ', 'Ꭿ', 'Ꮃ', 'Ꮑ', 'Ꮕ', 'Ꮪ', 'Ᏻ', 'ᐃ', 'ᐅ', 'ᐊ', 'ᐲ', 'ᐸ', 'ᑎ', 'ᑐ', 'ᑕ', 'ᑦ', 'ᑯ', 'ᑲ', 'ᒃ', 'ᒍ', 'ᒥ', 'ᒧ', 'ᓂ', 'ᓇ', 'ᓐ', 'ᓕ', 'ᓗ', 'ᓪ', 'ᓯ', 'ᓱ', 'ᓴ', 'ᔪ', 'ᔭ', 'ᕆ', 'ᕈ', 'ᕋ', 'ᕐ', 'ᖃ', 'ᚠ', 'ᚢ', 'ᚦ', 'ᚱ', 'ᚴ', 'ᚾ', 'ᛁ', 'ᛅ', 'ᛋ', 'ᛏ', 'ᛒ', 'ᛘ', 'ᛚ', 'ក', 'ខ', 'គ', 'ង', 'ច', 'ជ', 'ញ', 'ដ', 'ឌ', 'ណ', 'ត', 'ថ', 'ទ', 'ធ', 'ន', 'ប', 'ផ', 'ព', 'ភ', 'ម', 'យ', 'រ', 'ល', 'វ', 'ស', 'ហ', 'ឡ', 'អ', 'ឬ', 'ឯ', 'ា', 'ិ', 'ី', 'ឹ', 'ឺ', 'ុ', 'ូ', 'ួ', 'ើ', 'ៀ', 'េ', 'ែ', 'ៃ', 'ោ', 'ៅ', 'ំ', 'ះ', 'ៈ', '់', '៍', '័', '្', '។', 'ᠠ', 'ᠨ', 'ᠪ', 'ᠮ', 'ᠯ', 'ᡝ', 'ᡠ', 'ᡤ', 'ᡥ', 'ᡨ', 'ᡩ', 'ᡳ', 'ᡵ', 'ᴀ', 'ᴄ', 'ᴅ', 'ᴇ', 'ᴊ', 'ᴋ', 'ᴍ', 'ᴏ', 'ᴘ', 'ᴛ', 'ᴜ', 'ᴠ', 'ᴡ', 'ᴱ', 'ᴹ', 'ᴾ', 'ᵐ', 'ᵖ', 'ᵗ', 'ᵛ', 'ᵱ', 'ᵽ', 'ᶈ', 'ḇ', 'Ḍ', 'ḍ', 'Ḏ', 'ḏ', 'ḗ', 'Ḡ', 'ḡ', 'Ḥ', 'ḥ', 'ḩ', 'Ḫ', 'ḫ', 'ḭ', 'ḱ', 'Ḳ', 'ḳ', 'Ḵ', 'ḵ', 'ḷ', 'ṁ', 'Ṃ', 'ṃ', 'Ṅ', 'ṅ', 'Ṇ', 'ṇ', 'ṉ', 'ṓ', 'Ṕ', 'ṕ', 'Ṗ', 'ṗ', 'Ṛ', 'ṛ', 'ṝ', 'ṡ', 'Ṣ', 'ṣ', 'Ṭ', 'ṭ', 'Ṯ', 'ṯ', 'ṽ', 'ṿ', 'ẍ', 'ẏ', 'Ẓ', 'ẓ', 'ẗ', 'ạ', 'ả', 'Ấ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'Ế', 'ế', 'Ề', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'Ọ', 'ọ', 'ỏ', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ớ', 'ờ', 'ở', 'ợ', 'ụ', 'ủ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỷ', 'Ỹ', 'ỹ', 'ἀ', 'ἁ', 'ἂ', 'ἄ', 'ἅ', 'Ἀ', 'Ἁ', 'Ἄ', 'Ἅ', 'ἐ', 'ἑ', 'ἔ', 'ἕ', 'Ἐ', 'Ἑ', 'Ἕ', 'ἠ', 'ἡ', 'ἢ', 'ἣ', 'ἤ', 'ἥ', 'ἦ', 'ἧ', 'Ἠ', 'Ἡ', 'Ἥ', 'ἰ', 'ἱ', 'ἴ', 'ἵ', 'ἶ', 'ἷ', 'Ἰ', 'Ἱ', 'Ἴ', 'ὀ', 'ὁ', 'ὃ', 'ὄ', 'ὅ', 'Ὀ', 'Ὁ', 'Ὃ', 'Ὄ', 'Ὅ', 'ὐ', 'ὑ', 'ὓ', 'ὔ', 'ὕ', 'ὖ', 'ὗ', 'Ὑ', 'Ὓ', 'ὡ', 'ὣ', 'ὤ', 'ὥ', 'ὦ', 'Ὡ', 'ὰ', 'ὲ', 'ὴ', 'ὶ', 'ὸ', 'ὺ', 'ὼ', 'ᾖ', 'ᾳ', 'ᾴ', 'ᾶ', 'ᾷ', '᾽', '᾿', 'ῃ', 'ῄ', 'ῆ', 'ῇ', '῎', 'ῖ', 'ῥ', 'ῦ', 'Ῥ', 'ῳ', 'ῶ', 'ῷ', '῾', '\\u2002', '\\u2003', '\\u2009', '\\u200b', '‐', '‑', '‒', '–', '—', '―', '‖', '‗', '‘', '’', '‚', '‛', '“', '”', '„', '‟', '†', '‡', '•', '․', '…', '‧', '‰', '′', '″', '‴', '‵', '‶', '‷', '‹', '›', '※', '‼', '‽', '⁃', '⁄', '⁕', '⁗', '\\u2061', '\\u2062', '⁰', '⁴', '⁵', '⁶', '⁷', '⁸', '⁹', '⁺', '⁻', '₀', '₁', '₂', '₃', '₄', '₣', '₤', '₦', '€', '₱', '₵', '₹', '₿', '⃒', '⃣', '℃', '℉', 'ℏ', 'ℓ', '№', '℗', '℘', 'ℛ', 'ℝ', '℠', '™', '℥', 'ℹ', '⅓', '⅔', '⅕', '⅙', '⅛', '⅜', '⅝', 'Ⅰ', 'Ⅱ', 'Ⅲ', 'Ⅳ', 'Ⅴ', 'Ⅵ', 'Ⅶ', 'Ⅷ', 'ⅰ', 'ⅱ', 'ⅹ', 'ⅼ', '←', '↑', '→', '↓', '↔', '↗', '↘', '↠', '↩', '↪', '↴', '↵', '⇄', '⇌', '⇐', '⇒', '⇓', '⇔', '⇝', '⇧', '∀', '∂', '∃', '∅', '∆', '∇', '∈', '∎', '∏', '∑', '−', '∗', '∘', '∙', '√', '∛', '∜', '∝', '∞', '∠', '∥', '∧', '∨', '∩', '∫', '∮', '∴', '∵', '∶', '∼', '≃', '≅', '≈', '≒', '≔', '≠', '≡', '≤', '≥', '≦', '≧', '≪', '≫', '≲', '≻', '⊃', '⊆', '⊕', '⊗', '⊙', '⊢', '⊥', '⋀', '⋅', '⋯', '⌃', '⌊', '⌋', '⌐', '⌘', '⌛', '⍺', '⎜', '⎯', '⏰', '①', '②', '③', '④', '⑤', '⑥', '⑦', '⑧', '⑨', '⑩', '⑪', '⑫', 'Ⓡ', 'ⓒ', '⓵', '⓽', '─', '│', '┐', '└', '├', '┤', '┴', '║', '╗', '╝', '▀', '▄', '▐', '░', '▒', '▓', '■', '□', '▣', '▪', '▫', '▲', '▶', '▷', '▸', '►', '▻', '▼', '▽', '◀', '◆', '◇', '◊', '○', '◌', '●', '◙', '◦', '◮', '★', '☆', '☉', '☎', '☏', '☑', '☘', '☛', '☞', '☟', '☠', '☢', '☯', '☹', '☺', '☻', '☼', '♀', '♂', '♅', '♇', '♈', '♉', '♊', '♋', '♌', '♍', '♎', '♏', '♐', '♑', '♒', '♓', '♠', '♣', '♥', '♦', '♫', '♬', '♭', '♯', '♻', '♿', '⚑', '⚓', '⚙', '⚠', '⚡', '⛄', '⛈', '⛵', '✅', '✈', '✉', '✊', '✌', '✍', '✎', '✏', '✒', '✓', '✔', '✖', '✗', '✤', '✦', '✧', '✯', '✱', '✿', '❄', '❌', '❏', '❑', '❓', '❖', '❗', '❛', '❜', '❤', '❯', '❶', '➁', '➔', '➕', '➖', '➚', '➜', '➞', '➡', '➢', '➣', '➤', '➨', '➭', '⟨', '⟩', '⟶', '⟷', '⠀', '⦁', '⨉', '⩽', '⩾', '⬇', '⭐', 'Ⰲ', 'Ⰽ', 'Ⰾ', 'Ⱀ', 'Ⱁ', 'Ⱄ', 'Ⱏ', 'Ⱐ', 'Ⱑ', 'Ᵽ', 'ⱱ', 'Ⲡ', 'ⲡ', '⺇', '、', '。', '々', '〈', '〉', '《', '》', '「', '」', '『', '』', '【', '】', '〒', '〔', '〕', '〜', 'ぁ', 'あ', 'い', 'う', 'ぇ', 'え', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ひ', 'び', 'ふ', 'ぶ', 'へ', 'べ', 'ほ', 'ぼ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん', '゚', '゜', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ワ', 'ヰ', 'ヱ', 'ヲ', 'ン', 'ヴ', '・', 'ー', 'ㄎ', 'ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㇰ', 'ㇱ', 'ㇲ', 'ㇳ', 'ㇴ', 'ㇵ', 'ㇶ', 'ㇷ', 'ㇸ', 'ㇹ', 'ㇺ', 'ㇻ', 'ㇼ', 'ㇽ', 'ㇾ', 'ㇿ', '㎛', '㎢', '䖏', '一', '丁', '七', '万', '三', '上', '下', '不', '与', '丑', '专', '且', '丕', '世', '丘', '业', '东', '丝', '两', '严', '並', '个', '中', '丰', '串', '临', '丷', '丸', '丹', '为', '主', '乃', '久', '么', '义', '之', '乌', '乎', '乐', '乗', '乘', '乙', '九', '乞', '也', '习', '乡', '书', '乱', '乳', '乾', '亂', '了', '予', '争', '事', '二', '于', '云', '互', '五', '井', '亚', '些', '亞', '亟', '亡', '交', '产', '享', '京', '亭', '亮', '亲', '人', '亿', '什', '仁', '仅', '仇', '今', '介', '仍', '从', '仏', '仕', '他', '仗', '付', '仙', '仡', '代', '令', '以', '们', '仮', '仰', '件', '价', '任', '份', '仿', '伊', '伍', '伎', '伏', '伐', '休', '优', '伙', '会', '伝', '传', '伤', '伦', '伪', '伯', '伴', '伸', '似', '伽', '但', '佉', '位', '低', '住', '佐', '体', '何', '佘', '余', '佛', '作', '你', '佬', '佳', '使', '來', '例', '供', '依', '価', '侯', '侵', '便', '係', '促', '俄', '俊', '俗', '保', '信', '修', '俱', '俾', '倉', '個', '倍', '們', '倒', '候', '借', '値', '倪', '倭', '倶', '倻', '值', '假', '做', '停', '健', '側', '偽', '傅', '備', '傣', '储', '傳', '傾', '僉', '像', '僖', '僧', '儀', '億', '儆', '優', '儲', '儿', '元', '兄', '充', '先', '光', '克', '免', '児', '兑', '兒', '兔', '党', '兜', '入', '內', '全', '兩', '八', '公', '六', '兰', '共', '兲', '关', '兴', '兵', '其', '具', '典', '兼', '冀', '内', '円', '冈', '冊', '册', '再', '冎', '写', '军', '农', '冠', '冨', '冬', '冰', '冲', '决', '况', '冶', '准', '凉', '凋', '凍', '几', '凡', '凤', '処', '凰', '出', '击', '函', '刀', '分', '切', '刊', '刑', '划', '列', '刘', '则', '创', '初', '判', '別', '利', '刪', '别', '刮', '到', '制', '刷', '刺', '刻', '則', '前', '剎', '剛', '剣', '剤', '副', '割', '創', '劇', '劈', '劉', '劍', '力', '劝', '办', '功', '加', '务', '动', '助', '劫', '劬', '労', '勅', '勇', '勉', '勒', '動', '務', '勝', '勞', '募', '勢', '勤', '勧', '勸', '勺', '勿', '包', '化', '北', '匪', '匯', '匹', '区', '医', '匿', '區', '十', '千', '午', '半', '卍', '华', '协', '卐', '卓', '協', '卖', '南', '博', '占', '卦', '卯', '印', '即', '却', '卷', '厂', '压', '原', '厥', '厳', '去', '县', '参', '參', '又', '叉', '及', '友', '反', '发', '取', '受', '变', '叡', '叢', '口', '古', '句', '另', '叩', '只', '叫', '可', '台', '史', '右', '号', '司', '叹', '吃', '各', '合', '吉', '同', '名', '后', '吐', '向', '吕', '吗', '君', '否', '吧', '含', '听', '启', '吳', '吴', '吸', '吹', '吼', '呀', '告', '员', '呢', '周', '味', '呼', '命', '咄', '咋', '和', '咖', '咸', '咼', '哀', '品', '哈', '响', '員', '哥', '哦', '哪', '哲', '唐', '售', '唯', '商', '啊', '問', '啓', '啟', '啡', '啥', '啦', '啪', '善', '喚', '喜', '喝', '喪', '喫', '單', '営', '喷', '喻', '嗎', '嗣', '嘅', '嘉', '器', '嚴', '囉', '囍', '四', '回', '因', '团', '団', '园', '困', '図', '围', '国', '图', '圆', '國', '圏', '園', '圓', '圖', '圜', '土', '圣', '圧', '在', '圭', '地', '圳', '场', '坂', '均', '坊', '坏', '坐', '坑', '块', '坚', '坛', '坡', '型', '垢', '埃', '城', '埔', '域', '執', '培', '基', '埼', '堀', '堂', '堆', '堇', '堪', '報', '場', '塊', '塑', '塔', '塘', '塚', '塞', '填', '塵', '境', '墓', '増', '增', '墨', '墳', '壁', '壊', '壞', '士', '壬', '壮', '壯', '声', '売', '壹', '壺', '处', '备', '复', '夏', '外', '多', '夜', '够', '夠', '夢', '大', '天', '太', '夫', '央', '失', '头', '夷', '夾', '奇', '奉', '奏', '契', '奖', '套', '奘', '奚', '奥', '奧', '女', '奴', '她', '好', '如', '妃', '妄', '妇', '妈', '妖', '妘', '妙', '妹', '姉', '姊', '始', '姐', '姓', '委', '姜', '姫', '威', '娑', '娘', '娜', '娥', '娱', '婁', '婆', '婚', '婦', '媽', '嬢', '子', '孔', '字', '存', '孙', '孝', '孟', '季', '孤', '学', '孩', '孫', '學', '宁', '它', '宅', '宇', '守', '安', '宋', '完', '宍', '宗', '官', '宙', '定', '宛', '宜', '宝', '实', '実', '客', '宣', '室', '宥', '宫', '宮', '宰', '害', '家', '容', '宽', '宾', '宿', '寂', '寄', '寅', '密', '富', '寒', '寔', '寛', '寝', '察', '寡', '實', '寧', '審', '寫', '寬', '寰', '寶', '寸', '对', '寺', '寻', '导', '対', '寿', '封', '専', '射', '将', '將', '專', '尊', '對', '導', '小', '少', '尔', '尚', '尝', '尤', '就', '尸', '尹', '尻', '尼', '尽', '尾', '局', '居', '屆', '屈', '屋', '屍', '屎', '展', '属', '層', '履', '屬', '山', '岌', '岐', '岑', '岡', '岩', '岳', '岸', '峰', '島', '崇', '崎', '崔', '崛', '崤', '崩', '嵐', '嵩', '嵬', '嶋', '嶽', '巖', '川', '州', '工', '左', '巧', '巫', '差', '己', '已', '巳', '巴', '巻', '币', '市', '布', '帆', '师', '希', '帐', '帖', '帛', '帝', '帥', '带', '師', '席', '帮', '帯', '帰', '帳', '帶', '常', '幅', '幕', '幡', '幣', '干', '平', '年', '并', '幸', '幹', '幼', '广', '庁', '広', '庄', '庆', '庇', '序', '库', '应', '底', '店', '庚', '府', '度', '座', '庫', '庭', '庵', '庶', '康', '庸', '廊', '廟', '廢', '廣', '延', '建', '廻', '廿', '开', '弁', '异', '弃', '式', '弓', '引', '弗', '弟', '张', '弥', '弨', '張', '強', '弹', '强', '弾', '彌', '彎', '归', '当', '录', '彙', '彝', '彡', '形', '彦', '彩', '影', '役', '彼', '往', '征', '径', '待', '很', '律', '後', '徐', '徒', '従', '得', '從', '御', '復', '循', '微', '徳', '德', '徹', '徽', '心', '忄', '必', '忉', '忍', '志', '忘', '忙', '応', '忝', '忠', '快', '念', '忻', '忽', '忿', '态', '怎', '怛', '思', '急', '性', '怨', '怪', '总', '恋', '恒', '恨', '恩', '恭', '息', '恵', '悉', '悔', '悟', '患', '您', '悪', '悲', '悶', '情', '惊', '惑', '惟', '惠', '惡', '惣', '惯', '惱', '想', '惺', '愈', '意', '愛', '感', '愧', '愿', '慈', '慎', '慕', '慧', '慮', '慶', '憂', '憊', '憍', '憫', '憲', '應', '懷', '懿', '戀', '戈', '戊', '戏', '成', '我', '戒', '或', '战', '戦', '戰', '戲', '戴', '戶', '户', '戸', '房', '所', '扁', '扈', '手', '才', '扎', '打', '托', '扬', '扶', '批', '找', '承', '技', '抄', '把', '抒', '抓', '投', '抗', '折', '抣', '护', '报', '抱', '押', '抽', '拂', '担', '拉', '拍', '拓', '拔', '拘', '拙', '拜', '拥', '择', '括', '拳', '拼', '拾', '持', '指', '按', '振', '挹', '挿', '捉', '捕', '换', '捨', '据', '捲', '捺', '捻', '授', '掌', '掐', '排', '掛', '採', '探', '接', '推', '提', '插', '握', '損', '搭', '摄', '摘', '摧', '摩', '摸', '撒', '撫', '播', '擁', '擇', '擋', '攝', '支', '收', '改', '放', '政', '故', '救', '教', '敦', '敬', '数', '整', '文', '斌', '斎', '斐', '斗', '料', '斜', '斡', '斤', '断', '斯', '新', '方', '於', '施', '旃', '旅', '旋', '族', '旗', '无', '日', '旧', '旨', '早', '旬', '旭', '时', '旺', '昆', '昇', '昌', '明', '易', '昔', '星', '映', '春', '昧', '昨', '昭', '是', '昶', '显', '時', '晃', '晉', '晋', '晓', '晖', '晚', '普', '景', '智', '暑', '暖', '暗', '暦', '暫', '曇', '曉', '曲', '曳', '更', '書', '曹', '曼', '曽', '曾', '最', '會', '月', '有', '朋', '服', '望', '朝', '期', '木', '未', '末', '本', '札', '术', '朱', '朴', '机', '杀', '权', '杉', '李', '材', '村', '杜', '杞', '束', '条', '来', '杨', '杭', '杯', '杰', '東', '杵', '松', '板', '极', '构', '林', '果', '枝', '枣', '架', '枸', '柄', '某', '柔', '查', '柯', '柱', '柳', '柴', '査', '栄', '标', '树', '栖', '栗', '校', '栴', '样', '核', '根', '格', '桂', '桃', '案', '桓', '档', '梁', '梅', '條', '梨', '械', '梵', '检', '棄', '棉', '棒', '棚', '棟', '森', '椅', '植', '検', '楊', '楚', '楛', '楞', '業', '極', '楷', '楼', '楽', '概', '榓', '榕', '榜', '榮', '槃', '構', '様', '槟', '槻', '樂', '樑', '樓', '標', '樞', '模', '樣', '権', '横', '樹', '樺', '橋', '橛', '機', '檀', '檙', '檚', '檢', '櫛', '權', '次', '欢', '欣', '欲', '欽', '款', '歌', '止', '正', '此', '步', '武', '歯', '歲', '歳', '歴', '歷', '死', '殊', '残', '殖', '段', '殺', '殿', '毀', '母', '毎', '每', '毒', '比', '毘', '毛', '氏', '民', '气', '気', '氣', '水', '氵', '永', '汁', '求', '汉', '汗', '江', '池', '汪', '決', '汽', '沃', '沈', '沒', '沖', '沘', '沙', '没', '沪', '河', '沸', '油', '治', '況', '泄', '泉', '法', '泗', '波', '泥', '注', '泪', '泰', '泳', '洋', '洒', '洗', '洛', '津', '洲', '洹', '活', '派', '流', '浄', '浅', '浊', '济', '浒', '浙', '浜', '浦', '浩', '浪', '浮', '浴', '海', '涂', '涅', '消', '涌', '涙', '涤', '润', '涨', '淀', '淡', '淥', '淨', '淮', '深', '淳', '淵', '清', '済', '渊', '渋', '渔', '渕', '渠', '渡', '渤', '測', '港', '渴', '游', '湍', '湖', '湘', '湛', '湿', '満', '源', '準', '溥', '溪', '溫', '溯', '溶', '滅', '滇', '滑', '滔', '滞', '满', '滿', '漏', '漓', '演', '漚', '漢', '漫', '潤', '潮', '潰', '潼', '澀', '澄', '澤', '澳', '濁', '濃', '濟', '濱', '灌', '灣', '火', '灰', '灵', '灸', '炉', '炎', '炒', '炙', '炭', '点', '為', '烏', '烤', '烧', '热', '無', '焦', '然', '焼', '煌', '煎', '煚', '煤', '照', '煩', '煮', '熊', '熙', '熱', '熹', '燃', '燈', '燒', '燕', '營', '爆', '爱', '父', '爸', '爾', '牆', '片', '版', '牌', '牙', '牛', '牟', '牡', '牧', '物', '特', '犍', '犢', '犬', '状', '狂', '狐', '狩', '狭', '狮', '狼', '猗', '猪', '猫', '猿', '獄', '獏', '獨', '獲', '獻', '玄', '率', '玉', '王', '玖', '玩', '环', '现', '玲', '珍', '珎', '班', '現', '球', '琅', '理', '琉', '琴', '琿', '瑋', '瑎', '瑗', '瑛', '瑜', '瑞', '瑠', '瑾', '璃', '環', '璵', '瓊', '瓜', '瓦', '甄', '甘', '甚', '甜', '生', '産', '用', '田', '由', '甲', '申', '电', '男', '町', '画', '畅', '界', '畏', '留', '畢', '略', '番', '畫', '異', '當', '畿', '疏', '疑', '疫', '疯', '疾', '病', '症', '痛', '痧', '瘍', '瘦', '療', '癌', '癡', '癸', '発', '發', '白', '百', '的', '皆', '皇', '皮', '盂', '盆', '盛', '盡', '監', '盤', '盧', '目', '盲', '直', '相', '省', '看', '県', '真', '眠', '眦', '眼', '眾', '着', '睒', '睚', '睡', '督', '睦', '睺', '瞋', '瞎', '瞿', '矢', '知', '矩', '短', '石', '矿', '砂', '砌', '研', '砥', '砲', '破', '础', '硕', '硬', '确', '碑', '碧', '碱', '確', '磁', '磅', '磚', '磨', '礎', '礙', '示', '礼', '社', '祇', '祐', '祖', '祚', '祝', '神', '祠', '祥', '票', '祭', '祷', '禁', '禅', '禎', '福', '禘', '禪', '禮', '离', '禾', '秀', '私', '秉', '秋', '种', '科', '秒', '秘', '秦', '积', '称', '程', '稍', '稚', '稜', '種', '稱', '稲', '稽', '稿', '穂', '穆', '積', '究', '空', '突', '窃', '窄', '窑', '窓', '窟', '立', '站', '竜', '竞', '竟', '章', '童', '竭', '端', '竹', '竿', '笃', '笑', '符', '第', '筆', '等', '筍', '筒', '答', '策', '简', '箏', '算', '管', '節', '範', '篆', '篇', '築', '簡', '簧', '簿', '籀', '籍', '籏', '籠', '米', '类', '粉', '粒', '粘', '粟', '粤', '精', '糊', '糖', '糟', '系', '紀', '約', '紅', '紋', '紙', '級', '素', '索', '紫', '累', '細', '紹', '終', '組', '経', '結', '絕', '絞', '給', '統', '絲', '絵', '絹', '綏', '經', '継', '続', '綠', '綫', '維', '綱', '網', '綺', '綿', '総', '緑', '緒', '線', '緣', '編', '緯', '練', '縁', '縄', '縣', '縦', '縮', '縻', '總', '繁', '繋', '織', '繩', '纂', '續', '纖', '红', '纤', '约', '级', '纪', '纯', '纳', '纸', '纹', '纽', '线', '组', '绅', '细', '绍', '经', '结', '绘', '给', '统', '绮', '维', '综', '编', '缘', '缩', '网', '罗', '罪', '置', '署', '罹', '羅', '羈', '羊', '美', '群', '義', '羯', '羲', '羹', '翁', '翅', '習', '翟', '翠', '翻', '耀', '老', '考', '者', '耆', '而', '耐', '耨', '耳', '耶', '职', '联', '聖', '聘', '聚', '聞', '聮', '聯', '聰', '聲', '聴', '職', '聽', '肅', '肉', '肌', '肘', '肚', '肪', '肯', '育', '肺', '肿', '背', '胎', '胝', '胞', '胡', '胶', '胸', '能', '脂', '脈', '脉', '脏', '脑', '脚', '脫', '腊', '腔', '腕', '腦', '腰', '腸', '膀', '膚', '膝', '膳', '臍', '臚', '臟', '臣', '臥', '臨', '自', '至', '致', '臺', '臼', '與', '興', '舊', '舌', '舍', '舜', '舞', '航', '般', '船', '艇', '良', '艰', '色', '艺', '艾', '节', '芝', '芦', '花', '芸', '芹', '芽', '苏', '苑', '苔', '若', '苦', '英', '茂', '范', '茶', '荀', '草', '荏', '荒', '荣', '荷', '荼', '莎', '莒', '莫', '莲', '获', '菅', '菜', '菩', '華', '菲', '菴', '萁', '萌', '落', '葉', '著', '董', '葫', '葵', '蒙', '蒸', '蓄', '蓋', '蓓', '蓮', '蔘', '蔚', '蔡', '蔣', '蔭', '蔵', '蕴', '薄', '薇', '薩', '薫', '薬', '薯', '薰', '藉', '藍', '藏', '藐', '藝', '藤', '藩', '藻', '蘆', '蘇', '蘊', '蘋', '蘭', '虎', '虔', '處', '虛', '虞', '號', '虺', '虽', '蛇', '蛙', '蛮', '蜀', '蜂', '蜜', '蝮', '螺', '蠕', '蠡', '蠻', '血', '衅', '衆', '行', '衍', '術', '街', '衛', '衡', '衢', '衣', '补', '表', '袁', '袍', '袜', '被', '裂', '装', '裏', '裕', '裙', '補', '裝', '裤', '裴', '裸', '製', '褲', '襖', '襪', '襲', '西', '要', '見', '規', '視', '覚', '覧', '親', '覺', '觀', '见', '观', '规', '觅', '视', '览', '觉', '角', '解', '触', '言', '訂', '計', '訊', '討', '訓', '記', '訥', '設', '許', '訳', '訶', '証', '詁', '詔', '詞', '詢', '詣', '詩', '詮', '詰', '話', '該', '誉', '誌', '認', '誓', '誕', '語', '誠', '誥', '說', '説', '読', '課', '誼', '調', '談', '請', '諍', '論', '諜', '諦', '諭', '諲', '諸', '諺', '謀', '謂', '講', '謝', '證', '譔', '識', '譜', '譬', '議', '譲', '護', '讀', '變', '讓', '计', '订', '认', '讨', '让', '训', '议', '记', '许', '论', '设', '证', '诂', '评', '识', '诉', '诊', '词', '译', '诒', '试', '诗', '诚', '话', '该', '语', '误', '说', '请', '读', '课', '谁', '调', '谓', '谜', '谢', '谩', '谷', '豆', '豊', '豐', '象', '豪', '豫', '豹', '貊', '貘', '貝', '財', '貨', '貫', '責', '貴', '買', '費', '賀', '資', '賈', '賜', '賞', '賢', '賣', '賭', '購', '贅', '贛', '贡', '财', '贤', '败', '贴', '贵', '贸', '费', '贾', '资', '赋', '赖', '赛', '赞', '赢', '赣', '赤', '走', '赵', '赶', '起', '超', '越', '趕', '趙', '趣', '足', '跋', '跑', '距', '跡', '路', '践', '踊', '踏', '踐', '踵', '蹉', '蹴', '身', '車', '軌', '軍', '軟', '転', '軸', '軻', '載', '輔', '輕', '輝', '輩', '輪', '輸', '轉', '车', '轨', '轻', '载', '轿', '辆', '辑', '输', '辛', '辜', '辞', '辟', '辣', '辨', '辭', '辯', '辰', '辱', '農', '边', '辺', '込', '达', '迅', '过', '运', '近', '还', '这', '进', '远', '连', '迟', '迦', '迪', '述', '迴', '追', '退', '送', '适', '逆', '选', '透', '逐', '途', '這', '通', '逝', '速', '造', '連', '週', '進', '逸', '遊', '運', '遍', '過', '道', '達', '違', '遗', '遠', '遣', '適', '遮', '遵', '遷', '選', '遺', '遼', '邇', '邉', '邊', '邏', '邑', '邓', '那', '邦', '邪', '郁', '郎', '郝', '郡', '部', '郷', '都', '鄂', '鄉', '鄭', '配', '酢', '醋', '醎', '醒', '醣', '醫', '醬', '醯', '釁', '采', '釉', '释', '釋', '里', '重', '野', '量', '金', '釜', '鈥', '鈴', '鉄', '鉢', '鉴', '銀', '銃', '銅', '銘', '銷', '錄', '錍', '錢', '錦', '錫', '錬', '鍋', '鍛', '鍵', '鎌', '鎖', '鎮', '鏡', '鐵', '鑑', '钟', '钱', '铃', '铆', '铭', '铲', '铸', '销', '锅', '错', '镜', '長', '长', '門', '閉', '開', '閑', '閒', '間', '閗', '関', '閣', '閦', '閩', '閭', '閱', '閻', '闍', '闖', '關', '闥', '门', '闪', '闭', '问', '闲', '间', '闽', '阅', '队', '阪', '阳', '阻', '阼', '阿', '陀', '附', '际', '陇', '陈', '降', '限', '陕', '院', '陣', '除', '险', '陰', '陳', '陵', '陶', '陸', '陽', '隆', '隋', '隍', '階', '随', '隐', '隔', '際', '隨', '隱', '隶', '隸', '难', '雀', '雄', '雅', '集', '雑', '雕', '雜', '離', '難', '雨', '雪', '雲', '雷', '電', '需', '震', '霊', '霧', '露', '靈', '青', '靖', '静', '非', '面', '革', '靺', '鞋', '鞨', '韋', '韌', '韓', '韦', '韩', '音', '響', '頁', '頂', '須', '頊', '頌', '領', '頞', '頬', '頭', '頲', '頻', '頼', '顆', '題', '顏', '願', '類', '顧', '顯', '页', '顶', '项', '顺', '须', '预', '领', '颐', '频', '颗', '题', '颜', '额', '風', '风', '飛', '飞', '食', '飯', '飲', '飼', '養', '餐', '餓', '餖', '餘', '館', '餹', '饥', '饭', '饮', '饼', '馆', '首', '香', '馬', '駄', '駅', '駒', '駕', '駿', '騂', '騎', '騏', '験', '驅', '驗', '驚', '驪', '马', '驶', '验', '骑', '骤', '骨', '髓', '體', '高', '髙', '鬘', '鬱', '鬼', '魂', '魏', '魔', '魚', '魯', '鮮', '鯉', '鱼', '鱿', '鲜', '鳥', '鳩', '鳴', '鴞', '鴨', '鴻', '鵙', '鵜', '鵠', '鵲', '鶴', '鶻', '鶿', '鷲', '鷽', '鸟', '鸢', '鸬', '鹃', '鹰', '鹿', '麗', '麟', '麦', '麻', '麼', '黃', '黄', '黎', '黑', '黒', '默', '黙', '黨', '鼎', '鼓', '鼻', '齋', '齐', '龍', '龙', '龟', '龵', 'ꜥ', 'Ꝑ', 'ꝑ', 'Ꝓ', 'ꝓ', 'Ꝕ', 'ꝕ', 'ꟷ', 'ꟼ', 'ꦏ', 'ꦒ', 'ꦛ', 'ꦸ', 'ꦼ', '꧀', '가', '각', '간', '감', '갓', '강', '같', '개', '거', '건', '검', '것', '게', '겠', '겨', '격', '경', '계', '고', '골', '곰', '공', '과', '관', '광', '교', '구', '국', '군', '권', '규', '그', '근', '글', '금', '급', '기', '길', '김', '깁', '까', '께', '끝', '나', '낙', '난', '날', '남', '낮', '내', '너', '네', '넷', '년', '념', '노', '논', '농', '높', '놓', '뉴', '는', '늘', '능', '니', '닉', '님', '다', '단', '달', '담', '답', '대', '댐', '더', '덕', '데', '도', '동', '되', '됨', '됩', '두', '드', '든', '듣', '들', '듬', '등', '디', '따', '때', '떫', '떻', '또', '라', '락', '람', '래', '랫', '랭', '량', '러', '런', '럴', '럼', '럽', '렇', '레', '렌', '려', '력', '련', '렵', '령', '로', '록', '료', '루', '룹', '류', '륭', '르', '른', '를', '릉', '리', '린', '릴', '립', '마', '막', '만', '많', '말', '망', '맞', '맡', '매', '먼', '메', '며', '면', '멸', '명', '모', '목', '몬', '무', '문', '물', '뭔', '미', '민', '및', '바', '박', '반', '발', '방', '배', '백', '벅', '번', '범', '법', '베', '변', '보', '복', '본', '볼', '봅', '부', '북', '분', '불', '브', '비', '뿌', '사', '산', '살', '삶', '삽', '상', '새', '색', '생', '서', '석', '선', '설', '섭', '성', '세', '셀', '션', '소', '속', '손', '솟', '송', '쇠', '수', '숙', '순', '술', '쉽', '스', '슬', '습', '시', '식', '신', '실', '심', '십', '싶', '써', '씁', '씨', '아', '악', '안', '않', '았', '앞', '애', '액', '야', '약', '양', '어', '억', '엄', '업', '없', '었', '에', '여', '역', '연', '염', '엽', '였', '영', '예', '오', '온', '올', '와', '완', '왕', '외', '요', '용', '우', '운', '울', '움', '워', '원', '월', '위', '유', '육', '윤', '율', '융', '으', '은', '을', '음', '응', '의', '이', '인', '일', '읽', '임', '입', '있', '자', '작', '잘', '잡', '장', '재', '쟁', '저', '적', '전', '절', '점', '젓', '정', '젖', '제', '져', '조', '족', '존', '좀', '종', '좋', '주', '준', '중', '즈', '즉', '증', '지', '직', '진', '질', '집', '짓', '짜', '째', '창', '채', '책', '처', '천', '철', '첨', '청', '체', '초', '총', '최', '추', '축', '춘', '출', '측', '층', '치', '친', '카', '컨', '컴', '케', '코', '크', '클', '키', '킨', '킬', '타', '탁', '탄', '탕', '태', '탭', '터', '테', '템', '톤', '통', '퇴', '트', '특', '티', '틸', '팁', '파', '판', '팝', '펴', '편', '평', '포', '폼', '표', '품', '풍', '퓨', '플', '피', '하', '학', '한', '할', '함', '합', '항', '해', '핵', '행', '향', '허', '험', '현', '형', '호', '혹', '홉', '화', '확', '환', '활', '황', '회', '획', '효', '후', '훈', '휘', '힌', '\\uf020', '\\uf060', '\\uf062', '\\uf067', 'ﬀ', 'ﬁ', 'ﬂ', 'ﬃ', 'ﬆ', '﴾', '﴿', 'ﷺ', '︎', '️', '︠', '︡', '﹣', '！', '＆', '＇', '（', '）', '＊', '＋', '，', '－', '．', '／', '１', '２', '３', '４', '６', '７', '８', '：', '；', '＝', '？', 'Ａ', 'Ｂ', 'Ｃ', '［', '］', '～', 'ｮ', 'ﾍ', '￡', '￥', '￼', '�', '𐌐', '𐌰', '𐌲', '𐍀', '𐤐', '𓀐', '𓂝', '𓄈', '𓅓', '𝄐', '𝐄', '𝐑', '𝐖', '𝐚', '𝐜', '𝐡', '𝐢', '𝐦', '𝐨', '𝐫', '𝐭', '𝐰', '𝐱', '𝐴', '𝐵', '𝐶', '𝐷', '𝐹', '𝑓', '𝑘', '𝑣', '𝑥', '𝗫', '𝜏', '𝟏', '𝟷', '𝟹', '𝟽', '𝟿', '🇦', '🇧', '🇨', '🇪', '🇬', '🇮', '🇰', '🇳', '🇵', '🇸', '🇺', '🇿', '🌈', '🌊', '🌍', '🌎', '🌏', '🌱', '🌳', '🌵', '🌸', '🌿', '🍎', '🍓', '🍩', '🎀', '🎁', '🎃', '🎓', '🎛', '🎥', '🎧', '🎩', '🎮', '🎯', '🎵', '🎶', '🏃', '🏗', '🏦', '🏫', '🏳', '🏻', '🏼', '🏽', '🏾', '🐇', '🐘', '🐚', '🐛', '🐡', '🐥', '🐻', '🐾', '👁', '👂', '👃', '👄', '👇', '👉', '👋', '👌', '👍', '👏', '👑', '👦', '👧', '👨', '👩', '👶', '👸', '👽', '💃', '💌', '💐', '💔', '💙', '💚', '💞', '💡', '💪', '💬', '💭', '💯', '💰', '💲', '💵', '💸', '💼', '📃', '📉', '📊', '📌', '📍', '📑', '📕', '📖', '📗', '📚', '📜', '📝', '📢', '📩', '📹', '🔁', '🔎', '🔔', '🔗', '🔠', '🔢', '🔥', '🔬', '🔴', '🔵', '🔷', '🔸', '🔼', '🔽', '🕑', '🕸', '🖐', '🖨', '🗨', '🗳', '🗽', '😀', '😁', '😂', '😃', '😅', '😆', '😉', '😊', '😋', '😌', '😎', '😏', '😐', '😓', '😕', '😛', '😜', '😞', '😡', '😢', '😣', '😦', '😨', '😩', '😬', '😮', '😱', '😲', '😳', '😶', '🙁', '🙂', '🙄', '🙋', '🙌', '🙏', '🚀', '🚒', '🚗', '🚨', '🚲', '🚴', '🚸', '🛀', '🤑', '🤔', '🤖', '🤗', '🤞', '🤣', '🤦', '🤨', '🤩', '🤯', '🤴', '🤷', '🤿', '🥌', '🦪', '🧠', '🧡', '🪓', '𨽍'] 5614\n",
      "torch.Size([480032887])\n",
      "0.92555 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "sep = '.'\n",
    "max_samples = 100000\n",
    "samples = ds['train'][:max_samples]['text']\n",
    "text = sep.join(samples)\n",
    "print(len(samples), random.sample(samples, 5))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars, vocab_size)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.8140, val loss 8.8047\n",
      "step 100: train loss 3.0346, val loss 3.0546\n",
      "step 200: train loss 2.6364, val loss 2.6357\n",
      "step 300: train loss 2.4803, val loss 2.4836\n",
      "step 400: train loss 2.4034, val loss 2.4189\n",
      "step 500: train loss 2.3438, val loss 2.3495\n",
      "step 600: train loss 2.2790, val loss 2.3068\n",
      "step 700: train loss 2.2253, val loss 2.2543\n",
      "step 800: train loss 2.1913, val loss 2.2065\n",
      "step 900: train loss 2.1483, val loss 2.1914\n",
      "step 1000: train loss 2.1166, val loss 2.1553\n",
      "step 1100: train loss 2.0791, val loss 2.1330\n",
      "step 1200: train loss 2.0661, val loss 2.1045\n",
      "step 1300: train loss 2.0344, val loss 2.0772\n",
      "step 1400: train loss 1.9932, val loss 2.0504\n",
      "step 1500: train loss 1.9880, val loss 2.0342\n",
      "step 1600: train loss 1.9557, val loss 2.0331\n",
      "step 1700: train loss 1.9273, val loss 2.0047\n",
      "step 1800: train loss 1.9067, val loss 1.9883\n",
      "step 1900: train loss 1.9036, val loss 2.0164\n",
      "step 2000: train loss 1.8781, val loss 1.9837\n",
      "step 2100: train loss 1.8699, val loss 1.9717\n",
      "step 2200: train loss 1.8465, val loss 1.9815\n",
      "step 2300: train loss 1.8454, val loss 1.9651\n",
      "step 2400: train loss 1.8270, val loss 1.9515\n",
      "step 2500: train loss 1.8190, val loss 1.9488\n",
      "step 2600: train loss 1.8106, val loss 1.9192\n",
      "step 2700: train loss 1.7997, val loss 1.9056\n",
      "step 2800: train loss 1.7923, val loss 1.9150\n",
      "step 2900: train loss 1.7734, val loss 1.8866\n",
      "step 3000: train loss 1.7670, val loss 1.8950\n",
      "step 3100: train loss 1.7606, val loss 1.9026\n",
      "step 3200: train loss 1.7638, val loss 1.8956\n",
      "step 3300: train loss 1.7592, val loss 1.8825\n",
      "step 3400: train loss 1.7464, val loss 1.8781\n",
      "step 3500: train loss 1.7267, val loss 1.8711\n",
      "step 3600: train loss 1.7324, val loss 1.8811\n",
      "step 3700: train loss 1.7238, val loss 1.8615\n",
      "step 3800: train loss 1.7047, val loss 1.8729\n",
      "step 3900: train loss 1.6992, val loss 1.8596\n",
      "step 4000: train loss 1.7003, val loss 1.8545\n",
      "step 4100: train loss 1.7072, val loss 1.8533\n",
      "step 4200: train loss 1.6890, val loss 1.8392\n",
      "step 4300: train loss 1.6876, val loss 1.8329\n",
      "step 4400: train loss 1.6788, val loss 1.8432\n",
      "step 4500: train loss 1.6770, val loss 1.8367\n",
      "step 4600: train loss 1.6729, val loss 1.8436\n",
      "step 4700: train loss 1.6573, val loss 1.8181\n",
      "step 4800: train loss 1.6769, val loss 1.8346\n",
      "step 4900: train loss 1.6551, val loss 1.8246\n",
      "step 4999: train loss 1.6522, val loss 1.8206\n",
      "\n",
      "\n",
      ".QT\\S)\n",
      "=JHZIQJW FKKJW TS% GZY\n",
      "GJNSL FGTZSI \\J FS[NWYFYNTS+\n",
      "\n",
      "7@.4:)\n",
      "DTZ YMFY ZX HWT\\S'\n",
      "BFW\\NYM MTFYJ F\\F^% R^ KJFWX\n",
      "FW R^ YTZLMY\n",
      "=NHMRTTK NS MJFWY RNQJSI QNPJ JLWNJ[J&&\n",
      "-^ NS YMNX RFNI T[JWX% FSI BNQQ RF^ NS \\FWRX% QJQNSI RJFS'\n",
      "3T\\ R^ SJ\\ M^ \\JFU YMNX UQF\\J^'\n",
      "\n",
      ">4.494@>)\n",
      "\n",
      "?T YJQQ 4 \\NYM YMNX IJRFS)\n",
      "?MJ FQNSITS MJW J[NHJX YT YMTZ XFN[J\n",
      ",SI MNR MJ UTTU TK MNX GZY YMFY STGWZHY KTW FWJ\n",
      "?NX ST\\ YT YJUNSQJ TK \\TWIX ;WN[J R^ TKKTWI 4 XJJ!\n",
      "\n",
      "?=@.4:)\n",
      "\n",
      ">MTUUJW)\n",
      "4 FR YMTZLMY GZXNSL FWJ ST =WJUYNSL\n",
      ",SI KTW MNX SJ\\X FSP ^TZW R^ X\\JFSY ^TZW RFXYJW FSI XJJ&&\n",
      "6NRJSY \\J RJYMWTFI RNLMY GZY MJ \\NS YMJ GJLLJSY TK\n",
      "HTZSKJW^ YMFY YMFY QTWI\n",
      "4X FWJ TK 8FWNSL YMTZ J[JWJFGJSY T[JS F RTZSX\n",
      "@UJW TZW KFWJ[JY GJNSL) NS ITZGQT\\ KJ$Y'\n",
      ",X SJY HTR.QT\\* FXP SNLM% SJNHM MJ SJ\\% ]YMFY\n",
      "MJFWYM FGT\\S FLFNSXY ZSHJWFQ XTZQ'\n",
      "\n",
      ".QTRFS)\n",
      "BMFY XYTTQQ GJNSL NS MJWHM FI[NXJ\n",
      ",SI 4 NS MNR YMJ STYMTZLM YMNJ%\n",
      "?MTZLM TKK YNRJ YMJ WJXUJHYJ YT RTWJ'\n",
      "\n",
      ".,21TW \\MTR YMTZLMY HTZXJ YMNX GJQNPJ\n",
      ";TZSX MNR RTWJ NS RZHM YMNSJ'\n",
      "\n",
      ".7411 .,@70?)2T^% NS LNSJ \\J WTSJ% NY \\J MJF[J%\n",
      "BMJY 3JSWNHM' >MJ QT[J NS YMTZ% STY YFPJ FQQ STYM^ MTTXZ FSI\n",
      ",X GJ WJ\\NYMNSIX FSI JSJRNYNSL YMNSL \\TWYZSYJI%\n",
      ",SI YMJS% HMNQI KWNJSI! FQQ GJ \\4S\n",
      "-JKTWJ FSNSL FSI FSI YMJ FSLJI PJJU ITT\\! \\FW\\NYM YWNJX NS RJS\\NHP NY \\MNHM ^JFW \\NYM\n",
      "-JYYF IJYYM ^TZW 3J\\JQY YTT ZSQT\\X TK LWJFY\n",
      "TK YMJ \\TWOZXY IJKJXY% YMWTZ YMJW\n",
      ">MFQQ FSI GJ QT[J NS HFUJW$I% GZY RJS%\n",
      "1TW YMTZ \\NYM YM^ GJLZY+\n",
      "\n",
      "6492 309=D A4 A4 )\n",
      "2TI [NHP JQX\\JW \\MTR YMNSL \\NYM TSR ?MNX GJJS\n",
      "?T YMJQU UQFNSX ^NJQI RTXY FWJ \\TQQ YT\n",
      "YJQQ XRTSJXX'\n",
      "\n",
      "30=84:)\n",
      "8^ HWFWX% J[JX* KTW GZY TK F&RFYMJW RTYMJW'\n",
      "\n",
      ";:8,7:)\n",
      "7JY XTZQ\n",
      "?MFY XMTWJ GWJFPX% ^JFW NS TS R^ 7TWI'\n",
      "\n",
      "?MNWI 3:8;0D)\n",
      "9F^ RF^ RFWWNXM ZRGWTYMJW FWJ F LN[J%\n",
      "-JKTWJ% R^ UWNIJXYFYJ T$ ST% R^ MJFW\n",
      "-ZY FGQF^ STY% YMJWJ \\NKQT\\'\n",
      "\n",
      "6492 309=D 49.0 )\n",
      "2T! \\NYM FRTSL YWZT+\n",
      "\n",
      "7@.494@>)\n",
      "?MJWJ \\TZSI FKTTQ T$ TK YWZQI ZU ^TZ MJW%\n",
      "?T QNKJ MJ UZSNSY TZW TZY FWJ YFPJ*\n",
      ">FZR F \\NYM XF^% YW^ UWTRTZI YM^ F\\\\FX\n",
      "?MNSP F GWFZLMYJI KTW TS YMJ =TRFS!\n",
      "9TY GWN]Y XMT\\S$I KNYMJW RJ XZHM YT 8TSYMJW%\n",
      "?MTZ\n",
      "8FWH\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92555 M parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.6532, val loss 1.8181\n",
      "step 100: train loss 1.6100, val loss 1.7749\n",
      "step 200: train loss 1.6082, val loss 1.7859\n",
      "step 300: train loss 1.5929, val loss 1.7660\n",
      "step 400: train loss 1.6026, val loss 1.7600\n",
      "step 500: train loss 1.6005, val loss 1.7587\n",
      "step 600: train loss 1.5892, val loss 1.7757\n",
      "step 700: train loss 1.5917, val loss 1.7664\n",
      "step 800: train loss 1.5872, val loss 1.7548\n",
      "step 900: train loss 1.5916, val loss 1.7492\n",
      "step 999: train loss 1.5939, val loss 1.7651\n",
      "\n",
      "While if this own, which yet King Percuta,\n",
      "O evant Onvy the gate\n",
      "My Lordips unwick, tet? Frether ane away, my faftl's unzreound of the office your milend;\n",
      "Who eseigheds, I in latest in overs, and Warwick on you moself:\n",
      "This courtise wond my speak; and plaw you:\n",
      "My less Boopener'd gone:\n",
      "Give demberlal on in on him eigh-sound myself?\n",
      "\n",
      "TRUTUS:\n",
      "He good most forgued king thrust for tream. Why:\n",
      "Eold you ext the pair.\n",
      "\n",
      "COMINIUS:\n",
      "He-Now, art you are adsabraciang for him best any and\n",
      "Hirl to change carel's me: ank, I shall fair, gade?\n",
      "Is no and see--'Strange of thereful shame blood will we come,\n",
      "Thou courfe'st shout might. \n",
      "IEi have muckings.\n",
      "\n",
      "LUCIO:\n",
      "About overn impition\n",
      "Which is meen my see: in doubly a gRace,\n",
      "In this Clought untike to like tone.\n",
      "\n",
      "YORK:\n",
      "Maker beakes against uncestation with self monter so lady.\n",
      "\n",
      "PETRUMO:\n",
      "My Lord,\n",
      "As desider, poing Gentleman?\n",
      "Fy\n",
      "He this of time the ressence to me will;\n",
      "Good what those the sile Eill of keels as him\n",
      "For for crewatch time wort in forcest thee?\n",
      "\n",
      "Second Senater for all, I husbank;\n",
      "And her me again, corn till; I leave\n",
      "ay hong warres not rest mines and enemiting.\n",
      "\n",
      "GREMIO:\n",
      "O this faveded, and a borne and bring\n",
      "AInd my a caning and as ince any inkellows!\n",
      "\n",
      "COMINIUS:\n",
      "Let sir, but madisel have earted,\n",
      "And the stick for Hences to Tumlenslow!\n",
      "Arm'd life not just defe't, through he crown, I speak it is come abolouty done's the not come.\n",
      "Then they officer, the to garel vice else nothing him.\n",
      "\n",
      "RICHARD IV:\n",
      "O teny, delel! Here so, I am will raw old the\n",
      "else me is justice of plock, sages some the dealk\n",
      "What call but the see this true world\n",
      "By is thou Lord drieven hame, ar you\n",
      "No things to the mack a rishn. I will was I\n",
      "In distone coursel'd, by as though, this send; carl bring,\n",
      "There was and must thy banishmout cold, of mongly\n",
      "Your teard with the sent.\n",
      "\n",
      "GLOUCESTER:\n",
      "Bready will peepose: like he punser\n",
      "What is are the wask: and fance your years\n",
      "What now wonse than a bray. You sadn, Julio, parciol:\n",
      "In is shown'd for and a succe off.\n",
      "\n",
      "MINENIUS:\n",
      "Marr\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "# LM = Few-Shot Learners: Model Training Section\n",
    "# 1) Adam with β1 = 0.9, β2 = 0.95, and \u000f = 10−8\n",
    "# 2) Use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260\n",
    "# billion tokens, training continues at 10% of the original learning rate).\n",
    "# 3) There is a linear LR warmup over the first 375 million tokens.\n",
    "# 4) We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over\n",
    "# the first 4-12 billion tokens of training, depending on the model size. \n",
    "\n",
    "import math\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "max_batch_size = len(train_data) // block_size\n",
    "batch_inc_per_iter = (max_batch_size - batch_size) // max_iters\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, bs):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (bs,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, bs):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, bs)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# create a learning rate scheduler\n",
    "warmup_steps = int(0.1 * max_iters)\n",
    "decay_start = int(0.6 * max_iters)\n",
    "total_decay_steps = max_iters - decay_start\n",
    "\n",
    "# Create a scheduler with combined warmup and cosine decay\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "    lambda step: (step / warmup_steps) if step < warmup_steps else \n",
    "                 (0.1 + 0.9 * (1 + math.cos(math.pi * (step - decay_start) / total_decay_steps)) / 2) \n",
    "                 if step < decay_start else 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3846, val loss 4.3768\n",
      "step 100: train loss 3.3623, val loss 3.3901\n",
      "step 200: train loss 2.6645, val loss 2.6755\n",
      "step 300: train loss 2.3833, val loss 2.3892\n",
      "step 400: train loss 2.2128, val loss 2.2332\n",
      "step 500: train loss 2.0766, val loss 2.1168\n",
      "step 600: train loss 2.0318, val loss 2.0809\n",
      "step 700: train loss 1.9918, val loss 2.0512\n",
      "step 800: train loss 1.9589, val loss 2.0260\n",
      "step 900: train loss 1.9300, val loss 2.0063\n",
      "step 1000: train loss 1.9048, val loss 1.9874\n",
      "step 1100: train loss 1.8807, val loss 1.9720\n",
      "step 1200: train loss 1.8557, val loss 1.9550\n",
      "step 1300: train loss 1.8311, val loss 1.9402\n",
      "step 1400: train loss 1.8035, val loss 1.9240\n",
      "step 1500: train loss 1.7764, val loss 1.9066\n",
      "step 1600: train loss 1.7486, val loss 1.8884\n",
      "step 1700: train loss 1.7244, val loss 1.8742\n",
      "step 1800: train loss 1.6981, val loss 1.8563\n",
      "step 1900: train loss 1.6762, val loss 1.8428\n",
      "step 2000: train loss 1.6567, val loss 1.8307\n",
      "step 2100: train loss 1.6367, val loss 1.8135\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    batch_size += batch_inc_per_iter\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
